{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_chat.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxnLdeDgWcWH1IkFV4/kBe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maddarauci/speech_recognition/blob/main/Deep_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxNxZvfinBhF"
      },
      "source": [
        "import nltk \n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "import json \n",
        "import pickle \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD \n",
        "import random\n",
        "\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "data_file = open('intents.json').read()\n",
        "intents = json.loads(data_file)\n",
        "\n",
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    # word tokenization\n",
        "    w = nltk.word_tokenize(pattern)\n",
        "    words.extend(w)\n",
        "    # adding documents\n",
        "    documents.append((w, intent['tag']))\n",
        "\n",
        "    # adding classes to the classes list \n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])\n",
        "\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print(len(documents), 'Documents')\n",
        "print(len(classes), 'Classes')\n",
        "print(len(words), 'unique lemmatized words', words)\n",
        "\n",
        "pickle.dump(words,open('words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))\n",
        "\n",
        "# initializing the training data\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "for doc in documents:\n",
        "  #initializing the the bag of words\n",
        "  bag = []\n",
        "  # list of tokenized words for the pattern\n",
        "  pattern_words = doc[0]\n",
        "  # lemmatize each word - create base word, in attempt to represent related words \n",
        "  pattern_words =  [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "  # create our bag of words array with 1, if word match found in current pattern \n",
        "  for w in words:\n",
        "    bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "  # output is a '0' for each tag anf '1' for current tag (for each pattern)\n",
        "  output_row = list(output_empty)\n",
        "  output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "  training.append([bag, output_row]) \n",
        "\n",
        "# shuffle out features and turn into np.array \n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "# create train and test lists. X - patterns, Y - intents\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "print(\"training data created\")\n",
        "\n",
        "# create model - 3 layers. first layer 128 neurons, second 64 and 3rd output layer contains number of number \n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(.05))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "# compile the model. stochastic gradient descent with Nesterov accelerated gradient gives good results for this model \n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# fitting and saving the model\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5', hist) # h5 is a data saving file format \n",
        "\n",
        "\n",
        "print('\\nmodel created')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DCOGKJ9qtQV",
        "outputId": "ea81dc6a-b3a0-41af-d8fe-5ba4d71a189f"
      },
      "source": [
        "pip install tkinter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tkinter (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for tkinter\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "GXXNUx8yiacl",
        "outputId": "fecd0968-7b50-44d3-b908-7270e2d354f9"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import load_model\n",
        "model = load_model('chatbot_model.h5')\n",
        "import json\n",
        "import random\n",
        "intents = json.loads(open('intents.json').read())\n",
        "words = pickle.load(open('words.pkl','rb'))\n",
        "classes = pickle.load(open('classes.pkl','rb'))\n",
        "\n",
        "def clean_up_sentence(sentence):\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "  return sentence_words \n",
        "\n",
        "def bow(sentence, words, show_details=True):\n",
        "  #tokenize the pattern\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "  # baf of words - matrix of N words, vocab matrix\n",
        "  bag = [0]*len(words)\n",
        "  for s in sentence_words:\n",
        "    for i,w in enumerate(words):\n",
        "      if w == s:\n",
        "        # assign 1 if current word is in the vocab position\n",
        "        bag[i] = 1\n",
        "        if show_details:\n",
        "          print('found in bag: %s' % w)\n",
        "    return(np.array(bag))\n",
        "\n",
        "def predict_class(sentence, model):\n",
        "  # filter out prediction below a threshold \n",
        "  p = bow(sentence, words, show_details=False)\n",
        "  res = model.predict(np.array([p]))[0]\n",
        "  ERROR_THRESHOLD = 0.25\n",
        "  results.sort(key=lambda x: x[1], reverse=True)\n",
        "  return_list = []\n",
        "  for r in results:\n",
        "    return_list.append({'intent': classes[r[0]], 'probability': str([1])})\n",
        "  return return_list\n",
        "\n",
        "def getResponse(ints, intents_json):\n",
        "  tag = ints[0]['intent']\n",
        "  list_of_intents = intents_json['intents']\n",
        "  for i in list_of_intents:\n",
        "    if(i['tag']== tag):\n",
        "      result = random.choice(i['response'])\n",
        "      break\n",
        "  return result\n",
        "\n",
        "def chatbot_response(msg):\n",
        "  ints = predict_class(msg, model)\n",
        "  res = getResponse(ints, intents)\n",
        "  return res \n",
        "\n",
        "# creating the gui \n",
        "import tkinter as tk\n",
        "from tkinter import * \n",
        "import os, sys \n",
        "\n",
        "if os.environ.get('DISPLAY','') == '':\n",
        "    print('no display found. Using :0.0')\n",
        "    os.environ.__setitem__('DISPLAY', ':0.0')\n",
        "\n",
        "def send():\n",
        "  msg = EntryBox.get('1.0', 'end-lc').strip()\n",
        "  EntryBox.delete('0.0', END)\n",
        "\n",
        "  if msg != '':\n",
        "    ChatLog.config(state=NORMAL)\n",
        "    ChatLog.insert(END, \"U: \" + msg + '\\n\\n')\n",
        "    ChatLog.config(foreground=\"#442265\", font=(\"Verdana\", 12 ))\n",
        "\n",
        "    res = chatbot_response(msg)\n",
        "    ChatLog.insert(END, \"Bot: \" + res + '\\n\\n')\n",
        "\n",
        "    ChatLog.config(state=DISABLED)\n",
        "    ChatLog.yview(END)\n",
        "\n",
        "#root = Tkinter.Tk()\n",
        "root = tk.Tk()\n",
        "root.title(\"Feet to Meters\")\n",
        "\n",
        "root.geometry('400,500')\n",
        "root.resizeable(width=False, height=False)\n",
        "\n",
        "# creating window\n",
        "ChatLog = Text(root, bd=0, bg=\"white\", height=\"8\", width=\"50\", font=\"Arial\",)\n",
        "\n",
        "ChatLog.config(state=DISABLED)\n",
        "\n",
        "# bind scrollbar to chat window \n",
        "scrollbar = Scrollbar(root, command=ChatLog.yview, cursor='heart')\n",
        "ChatLog['yscrollcommand'] = scrollbar.set\n",
        "\n",
        "# create btn to send msg \n",
        "SendButton = Button(root, font=(\"Verdana\",12,'bold'), text=\"Send\", width=\"12\", height=5,\n",
        "                    bd=0, bg=\"#32de97\", activebackground=\"#3c9d9b\",fg='#ffffff',\n",
        "                    command= send)\n",
        "\n",
        "# create the box to enter msg \n",
        "EntryBox = Text(root, bd=0, bg=\"white\",width=\"29\", height=\"5\", font=\"Arial\")\n",
        "#EntryBox.bind(\"<Return>\", send)\n",
        "\n",
        "#Place all components on the screen\n",
        "scrollbar.place(x=376,y=6, height=386)\n",
        "ChatLog.place(x=6,y=6, height=386, width=370)\n",
        "EntryBox.place(x=128, y=401, height=90, width=265)\n",
        "SendButton.place(x=6, y=401, height=90)\n",
        "\n",
        "root.mainloop()\n",
        "\n",
        "print('\\ndone')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no display found. Using :0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c3b0fbf246a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m#root = Tkinter.Tk()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Feet to Meters\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: couldn't connect to display \":0.0\""
          ]
        }
      ]
    }
  ]
}